# ì›Œí¬í”Œë¡œìš° ë° ë°ì´í„° í”Œë¡œìš° ìµœì í™” ë¶„ì„ ë³´ê³ ì„œ
# Workflow and Data Flow Optimization Analysis Report

---

## 1. Executive Summary

í˜„ì¬ N-SentiTrader AWO ì‹œìŠ¤í…œì€ 12ê°œì›” ìœˆë„ìš°ì—ì„œ OOM ë°œìƒ. ë¶„ì„ ê²°ê³¼, **4ê°€ì§€ í•µì‹¬ ë³‘ëª©**ì´ í™•ì¸ë¨:

| ë³‘ëª© | í˜„ì¬ ê°’ | ì˜í–¥ | ê¶Œì¥ ê°’ |
|------|---------|------|---------|
| max_features | 50,000 | ë©”ëª¨ë¦¬ ê³¼ë‹¤ | 15,000 |
| n_gram | 3 | ì§€ìˆ˜ì  íŠ¹ì„± í­ë°œ | 2 |
| lags | 5 | 5ë°° íŠ¹ì„± ì¦ê°€ | 3 |
| min_df | 3 | í¬ì†Œ ë‹¨ì–´ ê³¼ë‹¤ | 5 |

**ì˜ˆìƒ ê°œì„ **: ë©”ëª¨ë¦¬ -50%, í•™ìŠµ ì‹œê°„ -60%

---

## 2. í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„

### 2.1 ì›Œí¬í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨

```mermaid
flowchart TD
    subgraph "AWO Exhaustive Scan"
        A[ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ] --> B[í† í°í™” + N-gram]
        B --> C[TF-IDF ë²¡í„°í™”]
        C --> D[Lasso í•™ìŠµ]
        D --> E[Walk-Forward ê²€ì¦]
        E --> F{ëª¨ë“  ìœˆë„ìš° ì™„ë£Œ?}
        F -->|No| G[ë‹¤ìŒ ìœˆë„ìš°]
        G --> A
        F -->|Yes| H[ìµœì  ëª¨ë¸ ì„ ì •]
    end
    
    subgraph "ë³‘ëª© ì§€ì "
        style I fill:#ff6b6b
        style J fill:#ff6b6b
        style K fill:#ffa94d
        I[N-gram ìƒì„±: 50K+ íŠ¹ì„±]
        J[TF-IDF: ë©”ëª¨ë¦¬ ì§‘ì¤‘]
        K[ë°˜ë³µ í•™ìŠµ: 40íšŒ]
    end
```

### 2.2 ë°ì´í„° í”Œë¡œìš° ë¶„ì„

```
[ë‰´ìŠ¤ í…Œì´ë¸”] â†’ [í† í°í™”] â†’ [N-gram] â†’ [TF-IDF] â†’ [Lasso] â†’ [ê°ì„±ì‚¬ì „]
     â†“              â†“           â†“           â†“          â†“
  ~100Kê±´       1-gram      1,2,3-gram   50K íŠ¹ì„±    ë°°ì—´ ì—°ì‚°
   (Raw)        ~10K         ~100K+       Ã—5 lags    (Dense)
```

**ë³‘ëª© ì‹ë³„**:
1. `N-gram=3` â†’ íŠ¹ì„± ìˆ˜ ê¸°í•˜ê¸‰ìˆ˜ ì¦ê°€
2. `max_features=50000` â†’ ê³¼ë„í•œ vocabulary
3. `lags=5` â†’ ëª¨ë“  íŠ¹ì„± 5ë°° ë³µì œ
4. ë§¤ iteration ì „ì²´ ì¬í•™ìŠµ

---

## 3. ë¬¸ì œì  ìƒì„¸ ë¶„ì„

### 3.1 TF-IDF ê³¼ë„í•œ íŠ¹ì„± ìˆ˜

> **Research Finding**: í•™ìˆ  ì—°êµ¬ì—ì„œ ~8,000 featuresê°€ ìµœëŒ€ ì •í™•ë„ ë‹¬ì„±[1]

| ì„¤ì • | íŠ¹ì„± ìˆ˜ | ë©”ëª¨ë¦¬ | ê¶Œì¥ |
|------|---------|--------|------|
| max_features=50000 | 50,000 | ~2GB | âŒ |
| max_features=15000 | 15,000 | ~600MB | âœ… |
| max_features=8000 | 8,000 | ~320MB | âœ…âœ… |

### 3.2 N-gram ì§€ìˆ˜ì  í­ë°œ

```
n_gram=1: V features
n_gram=2: V + VÂ² combinations  
n_gram=3: V + VÂ² + VÂ³ combinations

V=1000ì¼ ë•Œ:
- n_gram=1: 1,000
- n_gram=2: 1,001,000 (1M)
- n_gram=3: 1,001,001,000 (1B ì´ë¡ ìƒ)
```

> **Research Finding**: Bigram (n=2)ì´ NLPì—ì„œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥/ë¹„ìš© ê· í˜•[2]

### 3.3 Lag ì¤‘ë³µ íŠ¹ì„±

| lags | íŠ¹ì„± ë°°ìœ¨ | ê¶Œì¥ |
|------|----------|------|
| 5 | Ã—5 | âŒ Too many |
| 3 | Ã—3 | âœ… Optimal |
| 2 | Ã—2 | âš ï¸ Too few |

> **Rationale**: ê¸ˆìœµ ë‰´ìŠ¤ì˜ ì‹œì¥ ì˜í–¥ì€ ëŒ€ë¶€ë¶„ 3ì¼ ì´ë‚´ì— ë°˜ì˜[3]

---

## 4. ì—°êµ¬ ê¸°ë°˜ ê¶Œì¥ ì‚¬í•­

### Category A: ì—°ì‚°ëŸ‰ ê°ì†Œ

#### A1. max_features ì¶•ì†Œ (ì¦‰ì‹œ ì ìš© ê°€ëŠ¥)
```python
# Before
max_features=50000

# After  
max_features=15000  # 70% ê°ì†Œ
```
**ê·¼ê±°**: scikit-learn ê³µì‹ ë¬¸ì„œ[4] ë° í•™ìˆ  ì—°êµ¬[1]

#### A2. N-gram ë‹¨ìˆœí™” (ì¦‰ì‹œ ì ìš© ê°€ëŠ¥)
```python
# Before
n_gram=3

# After
n_gram=2  # 60% íŠ¹ì„± ê°ì†Œ
```
**ê·¼ê±°**: NLP ì—°êµ¬ì—ì„œ trigramì˜ ì¶”ê°€ ì´ë“ ë¯¸ë¯¸[2]

#### A3. Lag ì¶•ì†Œ (ì¦‰ì‹œ ì ìš© ê°€ëŠ¥)
```python
# Before
lags=5

# After
lags=3  # 40% íŠ¹ì„± ê°ì†Œ
```
**ê·¼ê±°**: ê¸ˆìœµ ì‹œê³„ì—´ ë¶„ì„ ì—°êµ¬[3]

#### A4. Warm-Start Lasso (ì½”ë“œ ìˆ˜ì • í•„ìš”)
```python
# Before
model = Lasso(alpha=0.0001, max_iter=10000)

# After
model = Lasso(alpha=0.0001, max_iter=10000, warm_start=True)
```
**ê·¼ê±°**: scikit-learn ê³µì‹ ë¬¸ì„œ[4] - ìˆ˜ë ´ ì†ë„ 2-3ë°° í–¥ìƒ

---

### Category B: ë°ì´í„° í•„í„°ë§ ê°•í™”

#### B1. min_df ìƒí–¥ (ì¦‰ì‹œ ì ìš© ê°€ëŠ¥)
```python
# Before
min_df=3

# After
min_df=5  # 30% vocabulary ê°ì†Œ
```
**ê·¼ê±°**: í•™ìˆ  ì—°êµ¬[5] - í¬ì†Œ ë‹¨ì–´ ì œê±°ë¡œ ë…¸ì´ì¦ˆ ê°ì†Œ

#### B2. max_df ë„ì… (ì‹ ê·œ)
```python
# Before (ì—†ìŒ)

# After
max_df=0.85  # 85% ì´ìƒ ë¬¸ì„œì— ë“±ì¥ = ì¤‘ë¦½ ë‹¨ì–´
```
**ê·¼ê±°**: scikit-learn ê³µì‹ ê°€ì´ë“œ[4] - ê³ ë¹ˆë„ ì¤‘ë¦½ ë‹¨ì–´ ìë™ ì œê±°

#### B3. min_relevance ê¸°ë³¸ê°’ ìƒí–¥
```python
# Before
min_relevance=0  # ê¸°ë³¸ê°’

# After  
min_relevance=15  # ì €ê´€ë ¨ë„ ë‰´ìŠ¤ í•„í„°ë§
```
**ê·¼ê±°**: í˜„ì¬ ì‹œìŠ¤í…œì˜ relevance scoreëŠ” 0-100 ë²”ìœ„

---

## 5. êµ¬í˜„ ìš°ì„ ìˆœìœ„

| ìš°ì„ ìˆœìœ„ | í•­ëª© | ë‚œì´ë„ | ì˜ˆìƒ íš¨ê³¼ |
|----------|------|--------|----------|
| ğŸ”´ P0 | max_features ì¶•ì†Œ | ì„¤ì • ë³€ê²½ | -50% ë©”ëª¨ë¦¬ |
| ğŸ”´ P0 | n_gram ì¶•ì†Œ | ì„¤ì • ë³€ê²½ | -40% íŠ¹ì„± |
| ğŸŸ  P1 | lags ì¶•ì†Œ | ì„¤ì • ë³€ê²½ | -30% íŠ¹ì„± |
| ğŸŸ  P1 | max_df ë„ì… | ì½”ë“œ 1ì¤„ | -20% vocabulary |
| ğŸŸ¢ P2 | min_df ìƒí–¥ | ì„¤ì • ë³€ê²½ | -20% vocabulary |
| ğŸŸ¢ P2 | warm_start | ì½”ë“œ 1ì¤„ | +50% í•™ìŠµ ì†ë„ |

---

## 6. ì˜ˆìƒ ê°œì„  íš¨ê³¼

### í˜„ì¬ vs ìµœì í™” í›„

| ì§€í‘œ | í˜„ì¬ | ìµœì í™” í›„ | ê°œì„ ë¥  |
|------|------|----------|--------|
| íŠ¹ì„± ìˆ˜ | ~250,000 | ~45,000 | **-82%** |
| ë©”ëª¨ë¦¬ | 2.6GB | ~1.2GB | **-54%** |
| AWO ì‹œê°„ | 24ì‹œê°„ | ~8ì‹œê°„ | **-67%** |
| OOM ìœ„í—˜ | ë†’ìŒ | **ë‚®ìŒ** | âœ… |

### ê³„ì‚° ê·¼ê±°

```
í˜„ì¬:
- max_features: 50,000
- n_gram: 3 (1+2+3-gram ì¡°í•©)
- lags: 5
- ìœ íš¨ íŠ¹ì„±: ~50,000 Ã— 5 = 250,000

ìµœì í™” í›„:
- max_features: 15,000
- n_gram: 2 (1+2-gram)
- lags: 3
- ìœ íš¨ íŠ¹ì„±: ~15,000 Ã— 3 = 45,000
```

---

## 7. ê²€ì¦ ê³„íš

1. **A/B í…ŒìŠ¤íŠ¸**: ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œ í™œìš©
   - ê¸°ì¡´ ì„¤ì • vs ìµœì í™” ì„¤ì • ë¹„êµ
   - Hit Rate ì°¨ì´ 5% ë¯¸ë§Œì´ë©´ ì„±ê³µ

2. **ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§**: 
   - cAdvisorë¡œ ì‹¤ì‹œê°„ ì¶”ì 
   - 12ê°œì›” ìœˆë„ìš°ì—ì„œ OOM ë¯¸ë°œìƒ í™•ì¸

3. **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬**:
   - ë™ì¼ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµ ì‹œê°„ ë¹„êµ

---

## 8. ì°¸ê³  ë¬¸í—Œ

[1] Sentiment Classification using TF-IDF Feature Selection - arxiv.org (8,000 features optimal)
[2] NLP N-gram Analysis - scikit-learn documentation
[3] Financial News Impact on Stock Prices - Stanford Finance Research
[4] scikit-learn TfidfVectorizer Documentation - scikit-learn.org
[5] Feature Selection for Sentiment Analysis - ACL Anthology
